{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a0b4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'assumed-role_voclabs_user1198264_siddhartha_maharana_gatech_edu', 'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1636217982374_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-50-116.ec2.internal:20888/proxy/application_1636217982374_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-3W2BYT44RYPMC\n",
       "\" application-id=\"application_1636217982374_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-62-120.ec2.internal:8042/node/containerlogs/container_1636217982374_0001_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630c8819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54b578272fd417895efd00adf4374b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Package already installed for current Spark context!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1110, in install_pypi_package\n",
      "    raise ValueError(\"Package already installed for current Spark context!\")\n",
      "ValueError: Package already installed for current Spark context!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\") #Install matplotlib from given PyPI repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97666d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c56bef145246488313066bda6d550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/92/96/144f70b972a9c0eabbd4391ef93ccd49d0f2747f4f6a2a2738e99e5adc65/requests-2.26.0-py2.py3-none-any.whl (62kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\" (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/04/a2/d918dcd22354d8958fe113e1a3630137e0fc8b44859ade3063982eacd2a4/idna-3.3-py3-none-any.whl (61kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/af/f4/524415c0744552cce7d8bf3669af78e8a069514405ea4fcbd0cc44733744/urllib3-1.26.7-py2.py3-none-any.whl (138kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\" (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/c8/820b1546c68efcbbe3c1b10dd925fbd84a0dda7438bc18db0ef1fa567733/charset_normalizer-2.0.7-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/45/946c02767aabb873146011e665728b680884cd8fe70dde973c640e45b775/certifi-2021.10.8-py2.py3-none-any.whl (149kB)\n",
      "Installing collected packages: idna, urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.7 idna-3.3 requests-2.26.0 urllib3-1.26.7\n",
      "\n",
      "You are using pip version 9.0.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6d53ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03564e7f803e47ab9f6cc6dc4cb2af52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1636218631812-0/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\"\n",
      "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.21.4 pandas-1.3.4\n",
      "\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "python37-sagemaker-pyspark 1.4.1 requires pyspark==2.3.4, which is not installed."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "#sc.uninstall_package('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16422039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c25fae520144912ad4b7ed154c6ef9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.0.0-py3-none-any.whl (954 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.19.0-py3-none-any.whl (356 kB)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in /mnt/tmp/1636218631812-0/lib/python3.7/site-packages (from selenium) (1.26.7)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: idna in /mnt/tmp/1636218631812-0/lib/python3.7/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyOpenSSL>=0.14; extra == \"secure\"\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: certifi; extra == \"secure\" in /mnt/tmp/1636218631812-0/lib/python3.7/site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Collecting cryptography>=1.3.4; extra == \"secure\"\n",
      "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/site-packages (from pyOpenSSL>=0.14; extra == \"secure\"->urllib3[secure]~=1.26->selenium) (1.13.0)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (427 kB)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: async-generator, attrs, outcome, sortedcontainers, sniffio, trio, h11, wsproto, trio-websocket, selenium, pycparser, cffi, cryptography, pyOpenSSL\n",
      "Successfully installed async-generator-1.10 attrs-21.2.0 cffi-1.15.0 cryptography-35.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 pycparser-2.21 selenium-4.0.0 sniffio-1.2.0 sortedcontainers-2.4.0 trio-0.19.0 trio-websocket-0.9.2 wsproto-1.0.0"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"selenium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c658ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048e1b620e704c6d8056e605263479d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /mnt/tmp/1636218631812-0/lib/python3.7/site-packages (from scipy) (1.21.4)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.7.2"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"scipy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36d36c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a73ef9dacb4515b518cced3fe10bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Classes to connect to music databse APIs.\"\"\"\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "#from data.utils import rate_limited\n",
    "\n",
    "\n",
    "class Connection():\n",
    "\n",
    "    \"\"\"Base connetion class.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\"\n",
    "        Inialize base connection.\n",
    "\n",
    "        Args\n",
    "            base_url: The base url of the API to connect to.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "\n",
    "    @rate_limited(2)\n",
    "    def query(self, query):\n",
    "        \"\"\"\n",
    "        Execute an API query.\n",
    "\n",
    "        query: String containing RESTful API query.\n",
    "        \"\"\"\n",
    "        return requests.get(self.base_url + query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WasabiConnection(Connection):\n",
    "\n",
    "    \"\"\"Class object instantiates a connection with Wasabi.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Wasabi API connection.\"\"\"\n",
    "        Connection.__init__(self, \"https://wasabi.i3s.unice.fr\")\n",
    "\n",
    "\n",
    "class AllMusicConnection(Connection):\n",
    "\n",
    "    \"\"\"Class object instantiates a connection with AllMusic.com.\"\"\"\n",
    "\n",
    "    def __init__(self, chromedriver_loc):\n",
    "        \"\"\"Initialize AllMusic.com connection.\"\"\"\n",
    "        Connection.__init__(self, \"https://www.allmusic.com\")\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        self.browser = webdriver.Chrome(\n",
    "            chromedriver_loc, chrome_options=chrome_options)\n",
    "\n",
    "    @rate_limited(2)\n",
    "    def query(self, query):\n",
    "\n",
    "        self.browser.get(self.base_url + query)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    conn = WasabiConnection()\n",
    "    r = conn.query(\"/search/fulltext/Bon%20Jovi\")\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4722a2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf73a9b14d0475ca778b125a61e4e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUR_DIR /\n",
      "ec2-user\n",
      "hadoop\n",
      "emr-notebook"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "MODULE_FULL_PATH = '/home/notebook/work'\n",
    "sys.path.insert(1, MODULE_FULL_PATH)\n",
    "\n",
    "import os\n",
    "CUR_DIR = os.getcwd()\n",
    "print(\"CUR_DIR\",CUR_DIR)\n",
    "#CUR_DIR='/home/notebook/work/data'\n",
    "CUR_DIR='/home'\n",
    "dirs = os.listdir(CUR_DIR)\n",
    "for file in dirs:\n",
    "   print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ca7d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16be93b7682c42599c8fe8dfc1930670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Method to rate limit API calls.\"\"\"\n",
    "\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from functools import wraps\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "def rate_limited(max_per_second):\n",
    "    \"\"\"\n",
    "    Rate limit function calls.\n",
    "\n",
    "    Courtesy of https://gist.github.com/gregburek/1441055\n",
    "    \"\"\"\n",
    "    lock = threading.Lock()\n",
    "    min_interval = 1.0 / float(max_per_second)\n",
    "\n",
    "    def decorate(func):\n",
    "        \"\"\"Decorate function.\"\"\"\n",
    "        last_time_called = [0.0]\n",
    "\n",
    "        @wraps(func)\n",
    "        def rate_limited_function(*args, **kwargs):\n",
    "            \"\"\"Rate limit function.\"\"\"\n",
    "            lock.acquire()\n",
    "            elapsed = time.time() - last_time_called[0]\n",
    "            left_to_wait = min_interval - elapsed\n",
    "\n",
    "            if left_to_wait > 0:\n",
    "                time.sleep(left_to_wait)\n",
    "\n",
    "            lock.release()\n",
    "\n",
    "            ret = func(*args, **kwargs)\n",
    "            last_time_called[0] = time.time()\n",
    "            return ret\n",
    "\n",
    "        return rate_limited_function\n",
    "\n",
    "    return decorate\n",
    "\n",
    "\n",
    "def item_user_matrix(X):\n",
    "    \"\"\"\n",
    "    Build the csr matrix of item x user.\n",
    "\n",
    "    Args\n",
    "        X: Dataframe with 'item', 'user', 'score' fields.\n",
    "\n",
    "    Return\n",
    "        item_user: csr matrix with items as rows, users as columns and score\n",
    "                   as values.\n",
    "        item_index: dictionary of the item indexes in the item_user matrix\n",
    "                    for each item id.\n",
    "        user_index: dicationary of the user indexes in the item_user matrix\n",
    "                    for each user_id.\n",
    "\n",
    "    ---------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    X['user_id'] = X['user_id'].astype(\"category\")\n",
    "    X['song_id'] = X['song_id'].astype(\"category\")\n",
    "\n",
    "    row = X['song_id'].cat.codes.copy()\n",
    "    col = X['user_id'].cat.codes.copy()\n",
    "\n",
    "    nrow = len(X['song_id'].cat.categories)\n",
    "    ncol = len(X['user_id'].cat.categories)\n",
    "\n",
    "    item_user = csr_matrix((X['score'], (row, col)), shape=(nrow, ncol))\n",
    "\n",
    "    user = dict(enumerate(X['user_id'].cat.categories))\n",
    "    user_index = {u: i for i, u in user.items()}\n",
    "\n",
    "    item = dict(enumerate(X['song_id'].cat.categories))\n",
    "    item_index = {s: i for i, s in item.items()}\n",
    "\n",
    "    return item_user, item_index, user_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ced2ca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d4adde79824fae90c2b6c8d94e9d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Downloaders for audio files.\"\"\"\n",
    "import pprint\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.shell import spark\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "#from data.utils import rate_limited\n",
    "#from data.connections import WasabiConnection\n",
    "#from data.connections import AllMusicConnection\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class Downloader():\n",
    "\n",
    "    \"\"\"Basic Downloader.\"\"\"\n",
    "\n",
    "    def __init__(self, save_loc):\n",
    "        \"\"\"Initialize downloader.\"\"\"\n",
    "        self.save_loc = save_loc\n",
    "\n",
    "    def download_file(self, url, file):\n",
    "        \"\"\"\n",
    "        Download the specified file to the save_loation directory.\n",
    "\n",
    "        Args\n",
    "            url: url where the file is located\n",
    "            file: the path of the file to save (including the extention)\n",
    "        \"\"\"\n",
    "        file_name = os.path.join(self.save_loc, file)\n",
    "        print(file)\n",
    "        print(file_name)\n",
    "        print(os.path.isfile(file))\n",
    "        if not os.path.isfile(file):\n",
    "            print(\"downloadfile \", url)\n",
    "            print(\"requesting\")\n",
    "            r = requests.get(url)\n",
    "            print(\"done\")\n",
    "            print(\"Saving {}\".format(file_name))\n",
    "            print(file)\n",
    "\n",
    "            with open(file, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "    def save_json(self, obj, file):\n",
    "        \"\"\"\n",
    "        Save object to the specified path in json format.\n",
    "\n",
    "        Args\n",
    "            obj: an object to save as json\n",
    "            file: the path of the file to save (including the extention)\n",
    "        \"\"\"\n",
    "        file_name = os.path.join(self.save_loc, file)\n",
    "\n",
    "        if not os.path.isfile(file):\n",
    "            with open(file, \"w\") as f:\n",
    "                print(\"Saving {}.json\".format(file_name))\n",
    "                json.dump(obj, f)\n",
    "\n",
    "\n",
    "class WasabiDownloader(Downloader):\n",
    "\n",
    "    \"\"\"Downloader for the Wasabi music database.\"\"\"\n",
    "\n",
    "    def __init__(self, save_loc, found_dir, notfound_dir):\n",
    "        \"\"\"\n",
    "        Initialize Downloader and crate connection to Wasabi API.\n",
    "\n",
    "        Args\n",
    "            save_loc: Directory that contains the folders found_dir and\n",
    "                      notfound_dir where the json and mp3 files will be saved.\n",
    "            found_dir: Directory name where found song data is saved.\n",
    "            notfound_dir: Directory name where not found song data is saved.\n",
    "        \"\"\"\n",
    "        print(\"WasabiDownloader\")\n",
    "        Downloader.__init__(self, save_loc)\n",
    "        self.conn = WasabiConnection()\n",
    "        self.save_loc = save_loc\n",
    "        self.found_loc = os.path.join(save_loc, found_dir)\n",
    "        self.notfound_loc = os.path.join(save_loc, notfound_dir)\n",
    "\n",
    "    @rate_limited(1)\n",
    "    def _search_artist(self, artist):\n",
    "        \"\"\"\n",
    "        Use the Wasabi API to search for the artistself.\n",
    "\n",
    "        Finds the best matching result and uses this artist. Rate limited to\n",
    "        @rate_limit(x)\n",
    "\n",
    "        Args\n",
    "            artist: name of the artist.\n",
    "\n",
    "        Return\n",
    "            Best matching result on Wasabi database for the given name of\n",
    "            artist.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # format artist name according to REST API requirements and make url\n",
    "        artist = quote(artist, safe='')\n",
    "        r = self.conn.query(\"/search/fulltext/\"+artist)\n",
    "\n",
    "        if r.content.find(b'Error') > -1:\n",
    "            return None\n",
    "        else:\n",
    "            r = r.json()\n",
    "\n",
    "        # return the best matching result found in \"namSuggest\" field\n",
    "        if r and 'nameSuggest' in r[0]:\n",
    "            best_match = r[0]['nameSuggest']['input'][0]\n",
    "            return best_match\n",
    "\n",
    "        return None\n",
    "\n",
    "    @rate_limited(1)\n",
    "    def _get_artist_data(self, artist):\n",
    "        \"\"\"\n",
    "        Use the Wasabi API to return the artist data provided by Wasabiself.\n",
    "\n",
    "        For an *exact* artist name that *exists* in the Wasabi database.\n",
    "        Rate limited to @rate_limit(x)\n",
    "\n",
    "        Args\n",
    "            artist: exact artist name that exists in the Wasabi database.\n",
    "\n",
    "        Return\n",
    "            artist data json return from the Wasabi API.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # format artist name according to REST API requirements and make url\n",
    "        artist = quote(artist, safe='')\n",
    "        r = self.conn.query(\"/search/artist/\"+artist)\n",
    "        print(\"r \",r)\n",
    "        artist_data = r.json()\n",
    "\n",
    "        if 'error' in artist_data:\n",
    "            return None\n",
    "\n",
    "        return artist_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_artist_songs(artist_data):\n",
    "        \"\"\"\n",
    "        Collect the songs from a given artist data json.\n",
    "\n",
    "        Args\n",
    "            artist_data: artist data json return from the Wasabi API.\n",
    "\n",
    "        Return\n",
    "            dataframe of artist, album and songs.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        albums = artist_data['albums']\n",
    "        songs = defaultdict(list)\n",
    "        for album in albums:\n",
    "            for song in album['songs']:\n",
    "\n",
    "                songs['artist_id'] += [artist_data['_id']]\n",
    "                songs['artist'] += [artist_data['name']]\n",
    "                songs['album'] += [album['title']]\n",
    "                songs['album_id'] += [album['_id']]\n",
    "                songs['title_lower'] += [str(song['title']).lower()]\n",
    "\n",
    "                for k, v in song.items():\n",
    "                    songs[k] += [v]\n",
    "\n",
    "        return pd.DataFrame(songs)\n",
    "\n",
    "    @rate_limited(2)\n",
    "    def _get_song_data(self, song_id):\n",
    "        \"\"\"\n",
    "        Use a Wasabi song id to retun the song data from the Wasabi API.\n",
    "\n",
    "        Args\n",
    "            wasabi_song_id: a song id from the Wasabi API.\n",
    "\n",
    "        Return\n",
    "            song data json returned from the Wasabi API.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # call the rest api and convert to json\n",
    "        success = False\n",
    "        while success is False:\n",
    "            r = self.conn.query(\"/api/v1/song/id/\"+song_id)\n",
    "            if r.content == b'Too many requests, please try again later.':\n",
    "                success = False\n",
    "                time.sleep(20)\n",
    "            else:\n",
    "                success = True\n",
    "                song_data = r.json()\n",
    "\n",
    "        return song_data\n",
    "\n",
    "    def scrape(self, track_df):\n",
    "        \"\"\"\n",
    "        Download previews and save song data json for tracks from track_df.\n",
    "\n",
    "        Args\n",
    "            track_df: Dataframe of the tracks that should be processed.  Must\n",
    "                      contain the fields \"artist\", \"title_lower\", and\n",
    "                      \"song_id\".\n",
    "            save_loc: Directory that contains the folders \"clips\" and \"noclips\"\n",
    "                      where the json and mp3 files will be saved.\n",
    "\n",
    "        Return\n",
    "            saves the song data json and mp3 files for previews.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # artists = [str(row.artist_name) for row in track_df.select(\"artist_name\").collect()]\n",
    "        track_df = track_df.withColumnRenamed(\"artist_name\",\"artist\").distinct()\n",
    "        track_df = track_df.toPandas()\n",
    "\n",
    "        def run(string):\n",
    "\n",
    "            # Make own character set and pass\n",
    "            # this as argument in compile method\n",
    "            regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "            # Pass the string in search\n",
    "            # method of regex object.\n",
    "            if (regex.search(string) == None):\n",
    "                return string\n",
    "\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        artists = np.sort(track_df['artist'].dropna().unique())\n",
    "        artists = [run(artist) for artist in artists]\n",
    "        artists = list(filter(None, artists))\n",
    "\n",
    "        print(\"artists:\", len(artists))\n",
    "        # search each artist\n",
    "        for artist in artists:\n",
    "            artist_data = None\n",
    "            intersect = None\n",
    "            print(\"starting artist \", artist)\n",
    "            artist = self._search_artist(artist.lower())\n",
    "            print(\"artist found from api \", artist)\n",
    "            msd_songs = track_df[track_df['artist'] == artist]\n",
    "            if not msd_songs.empty:\n",
    "                msd_songs_df = spark.createDataFrame(msd_songs)\n",
    "                msd_songs_df.show()\n",
    "            if artist is not None:\n",
    "                artist_data = self._get_artist_data(artist)\n",
    "                print(\"found artist data \",artist_data)\n",
    "\n",
    "            if artist_data is not None:\n",
    "                songs_df = spark.createDataFrame(self._get_artist_songs(artist_data))\n",
    "                songs_df.show(10,False)\n",
    "                intersect = msd_songs.merge(\n",
    "                    self._get_artist_songs(artist_data),\n",
    "                    on=['title_lower']).dropna()\n",
    "                if not intersect.empty:\n",
    "                    intersect_df = spark.createDataFrame(intersect)\n",
    "                    intersect_df.show(10,False)\n",
    "\n",
    "            # add to track dict for each song in both data sets\n",
    "\n",
    "            if intersect is not None and intersect.shape[0] > 0:\n",
    "                print(\"here\")\n",
    "                print(len(intersect))\n",
    "\n",
    "                for _, row in intersect.iterrows():\n",
    "\n",
    "                    mp3_file = \"{}.mp3\".format(row['song_id'])\n",
    "                    mp3_exists = os.path.isfile(\n",
    "                        os.path.join(self.found_loc, mp3_file))\n",
    "\n",
    "                    json_file = \"{}.json\".format(row['song_id'])\n",
    "                    json_exists = os.path.isfile(\n",
    "                        os.path.join(self.found_loc, json_file))\n",
    "\n",
    "                    track = self._get_song_data(row['_id'])\n",
    "                    track['song_id'] = row['song_id']\n",
    "\n",
    "                    if track['preview']:\n",
    "                        self.save_loc = self.found_loc\n",
    "                        if not mp3_exists:\n",
    "                            self.download_file(track['preview'], mp3_file)\n",
    "                        if not json_exists:\n",
    "                            self.save_json(track, json_file)\n",
    "                    else:\n",
    "                        self.save_loc = self.notfound_loc\n",
    "                        print(\"No preview for {} by {}\".\n",
    "                              format(track['title'], artist))\n",
    "                        if not json_exists:\n",
    "                            self.save_json(track, json_file)\n",
    "\n",
    "    def _get_songs_byindex(self, start_index):\n",
    "        \"\"\"\n",
    "        Use a start index to retun the song data from the Wasabi API.\n",
    "\n",
    "        Args\n",
    "            wasabi_song_id: a song id from the Wasabi API.\n",
    "\n",
    "        Return\n",
    "            song data json returned from the Wasabi API.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # call the rest api and convert to json\n",
    "        success = False\n",
    "        while success is False:\n",
    "            r = self.conn.query(\"/api/v1/song_all/\"+str(start_index))\n",
    "            if r.content == b'Too many requests, please try again later.':\n",
    "                success = False\n",
    "                time.sleep(20)\n",
    "            else:\n",
    "                success = True\n",
    "                song_data = r.json()\n",
    "\n",
    "        return song_data\n",
    "\n",
    "\n",
    "class AllMusicDownloader(Downloader):\n",
    "\n",
    "    \"\"\"Downloader for AllMusic.com.\"\"\"\n",
    "\n",
    "    def __init__(self, save_loc, chromedriver_loc):\n",
    "        \"\"\"\n",
    "        Initialize the downloader.\n",
    "\n",
    "        Args\n",
    "            save_loc: location to save the downloaded bios.\n",
    "            chromedriver_loc: path to the chromedriver.exe\n",
    "        \"\"\"\n",
    "        Downloader.__init__(self, save_loc)\n",
    "        self.conn = AllMusicConnection(chromedriver_loc)\n",
    "        self.save_loc = save_loc\n",
    "\n",
    "    def _goto_artist_from_song(self, song_query):\n",
    "        \"\"\"\n",
    "        Navigate to the artist page from a song page string.\n",
    "\n",
    "        Args\n",
    "            song_query: the last part of the song page url.\n",
    "        \"\"\"\n",
    "        self.conn.query(\"/song/\"+quote(song_query, safe=''))\n",
    "        WebDriverWait(self.conn.browser, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"song-artist\")))\n",
    "        soup = BeautifulSoup(self.conn.browser.page_source, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode_contents(), 'lxml')\n",
    "        soup = soup.find(\"h2\", {\"class\": \"song-artist\"})\n",
    "        artist_link = soup.find(\"a\")[\"href\"]\n",
    "        self.conn.browser.get(artist_link)\n",
    "\n",
    "    def _soup_artist_page(self):\n",
    "        \"\"\"\n",
    "        Soup the artist page.\n",
    "\n",
    "        Return\n",
    "            Soup of the artist page\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(self.conn.browser.page_source, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode_contents(), 'lxml')\n",
    "        return soup\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_artist_shortbio(artist_page_soup):\n",
    "        \"\"\"\n",
    "        Extract the artist short bio.\n",
    "\n",
    "        Args\n",
    "            artist_page_soup: soup of the artist page html.\n",
    "\n",
    "        Return\n",
    "            (string) artist short bio.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        return artist_page_soup.find(\"span\", {\"itemprop\": \"reviewBody\"}).text\n",
    "\n",
    "    def _get_artist_longbio(self, artist_page_soup):\n",
    "        \"\"\"\n",
    "        Extract the artist short bio.\n",
    "\n",
    "        Args\n",
    "            artist_page_soup: soup of the artist page html.\n",
    "\n",
    "        Return\n",
    "            (list of strings) each list element corresponds to a paragraph.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        bio_link = artist_page_soup.find(\n",
    "            \"p\", {\"class\": \"biography\"}).find(\"a\")[\"href\"]\n",
    "        self.conn.browser.get(bio_link)\n",
    "        WebDriverWait(self.conn.browser, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"text\")))\n",
    "        soup = BeautifulSoup(self.conn.browser.page_source, 'html.parser')\n",
    "        soup = BeautifulSoup(soup.decode_contents(), 'lxml')\n",
    "        soup = soup.find(\"div\", {\"itemprop\": \"reviewBody\"})\n",
    "        long_bio = [x.strip() for x in soup.text.split(\"\\n\")\n",
    "                    if (len(x.strip()) > 0 and x.find(\"<\") == -1)]\n",
    "        return long_bio\n",
    "\n",
    "    def scrape(self, songlinks_df):\n",
    "        \"\"\"\n",
    "        Scrape to collect artist short and long bios.\n",
    "\n",
    "        Args\n",
    "            songlinks_df: a dataframe with columns 'song_id' and 'url_allmusic'\n",
    "\n",
    "        Return\n",
    "            (dataframe) saves original dataframe with short and long bios.\n",
    "\n",
    "        -----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        total = songlinks_df.shape[0]\n",
    "        progress = tqdm.tqdm(total=total)\n",
    "        short_bios = []\n",
    "        long_bios = []\n",
    "        for _, row in songlinks_df.iterrows():\n",
    "            try:\n",
    "                song_url = row['url_allmusic'].split(\"/\")[-1]\n",
    "                song_id = row['song_id']\n",
    "                try:\n",
    "                    self._goto_artist_from_song(song_url)\n",
    "                    artist_page_soup = self._soup_artist_page()\n",
    "                    short_bios += [self._get_artist_shortbio(artist_page_soup)]\n",
    "                    long_bios += [self._get_artist_longbio(artist_page_soup)]\n",
    "                except Exception:\n",
    "                    print(\"Did not process {}\".format(song_id))\n",
    "                    short_bios += [None]\n",
    "                    long_bios += [None]\n",
    "                progress.update(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Saving current progress...\")\n",
    "                songlinks_df['short_bio'] = short_bios + \\\n",
    "                    [None]*(total-len(short_bios))\n",
    "                songlinks_df['long_bio'] = long_bios + \\\n",
    "                    [None]*(total-len(long_bios))\n",
    "                songlinks_df.to_csv(self.save_loc, index=False)\n",
    "                self.conn.browser.quit()\n",
    "\n",
    "        songlinks_df['short_bio'] = short_bios\n",
    "        songlinks_df['long_bio'] = long_bios\n",
    "        songlinks_df.to_csv(self.save_loc, index=False)\n",
    "        self.conn.browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6867c4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6beba1bd67404bb2e7461b90df6ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'Path does not exist: hdfs://ip-172-31-50-116.ec2.internal:8020/data/train_triplets1.txt;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 476, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: hdfs://ip-172-31-50-116.ec2.internal:8020/data/train_triplets1.txt;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to download audio files.\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, col, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "#from data.downloaders import WasabiDownloader\n",
    "\n",
    "\n",
    "def get_visited_song_ids(msd_dir):\n",
    "    \"\"\"\n",
    "    Build a list of the previously processed MSD song idsself.\n",
    "\n",
    "    Used so that data collection can resume if it is interrupted.\n",
    "\n",
    "    Args\n",
    "        msd_dir: Directory that contains the clips and the noclips folders\n",
    "        where the mp3 and song data json are saved.\n",
    "\n",
    "    Return\n",
    "        list of the MSD song ids that have already been processed.\n",
    "\n",
    "    ---------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for f in glob.glob(os.path.join(msd_dir, \"clips\", \"*.json\")):\n",
    "        ids += [f.split(\"/\")[-1].split(\".\")[0]]\n",
    "\n",
    "    for f in glob.glob(os.path.join(msd_dir, \"noclips\", \"*.json\")):\n",
    "        ids += [f.split(\"/\")[-1].split(\".\")[0]]\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"project\") \\\n",
    "    .config(\"...\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "CUR_DIR = os.getcwd()\n",
    "\n",
    "file = os.path.join(CUR_DIR, \"data\", \"unique_tracks.txt\")\n",
    "\n",
    "\n",
    "# load taste profile dataset\n",
    "file = os.path.join(CUR_DIR, \"data\", \"train_triplets1.txt\")\n",
    "colnames = ['user_id', 'song_id', 'playcount']\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"playcount\", IntegerType(), True)])\n",
    "taste_df = spark.read.csv(file, header=False,sep='\\t',schema=schema)\n",
    "taste_df.printSchema()\n",
    "# load msd track info\n",
    "file = os.path.join(CUR_DIR, \"data\", \"unique_tracks.txt\")\n",
    "\n",
    "track_df = spark.read.csv(file, header=True,sep=',').withColumnRenamed(\"song_id\",\"songid\")\n",
    "str_cols = [f.name for f in track_df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "print(str_cols)\n",
    "for c in str_cols:\n",
    "    track_df = track_df.withColumn(c, regexp_replace(col(c), '\"', \"\"));\n",
    "    track_df = track_df.withColumn(c, regexp_replace(col(c),  \"'\", \"\"));\n",
    "\n",
    "track_df.select(*str_cols).show(10,False)\n",
    "\n",
    "select_cols = ['song_id', 'track_id', 'artist_name', 'song_title']\n",
    "taste_df = taste_df.join(track_df, taste_df[\"song_id\"] == track_df[\"songid\"], \"inner\").select(*select_cols)\n",
    "taste_df.show()\n",
    "\n",
    "taste_df = taste_df.withColumn('title_lower', lower(col('song_title')))\n",
    "\n",
    "save_loc = os.path.join(CUR_DIR, \"audio\", \"MSD\")\n",
    "wasabi_downloader = WasabiDownloader(save_loc, 'clips', 'noclips')\n",
    "wasabi_downloader.scrape(taste_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770f94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
